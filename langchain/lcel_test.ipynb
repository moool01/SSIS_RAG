{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "#api key(추가해서 쓰시오)\n",
    "import settings\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.chains import RetrievalQA, HypotheticalDocumentEmbedder\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "import pickle\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = settings.openai_api_key\n",
    "\n",
    "# directory = os.path.dirname(__file__)\n",
    "# os.chdir(directory)\n",
    "\n",
    "def _device_check() : \n",
    "    ''' for check cuda availability '''\n",
    "    import torch\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # torch.backends.mps.is_available()\n",
    "    return device\n",
    "\n",
    "#llm\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=1)\n",
    "\n",
    "#embedding config\n",
    "embedding = SentenceTransformerEmbeddings(\n",
    "    model_name=\"BM-K/KoSimCSE-roberta-multitask\", \n",
    "    model_kwargs={'device':_device_check()}, \n",
    "    encode_kwargs={'normalize_embeddings':True},\n",
    "    )\n",
    "\n",
    "## embedding config - HyDE\n",
    "prompt_template = \"\"\" \n",
    "당신은 대한민국의 복지제도 전문가입니다. 복지 제도를 기반으로, 주어진 #질문에 #답변하면 됩니다.\n",
    "\n",
    "#질문 : {question}\n",
    "#답변 : ... \n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"question\"], template=prompt_template)\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "HyDEembeddings = HypotheticalDocumentEmbedder(\n",
    "    llm_chain=llm_chain,\n",
    "    base_embeddings=embedding,\n",
    ")\n",
    "\n",
    "#get vectorstore *HyDE Embedding\n",
    "vectorstore = Chroma(collection_name=\"vector_db\", persist_directory=\"./chroma_storage\", embedding_function=HyDEembeddings)\n",
    "\n",
    "# print(vectorstore.similarity_search(\"국가장학금\", k=3,))\n",
    "\n",
    "#get document from pickle(use as documents in bm25_retriever)\n",
    "with open('./document.pkl', 'rb') as file :\n",
    "    documents = pickle.load(file)\n",
    "\n",
    "#Vector Search Retriever\n",
    "chroma_retriever = vectorstore.as_retriever(search_type='mmr', search_kwargs={\"k\":5},)\n",
    "\n",
    "#BM25 Retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(documents=documents)\n",
    "bm25_retriever.k = 5\n",
    "\n",
    "#ensemble\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, chroma_retriever], weights=[0.5, 0.5],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이에 대한 정보는 제가 가지고 있지 않습니다.\n"
     ]
    }
   ],
   "source": [
    "#setup chain\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "                    llm=llm,\n",
    "                    chain_type=\"stuff\",\n",
    "                    retriever=ensemble_retriever,)\n",
    "\n",
    "# print(ensemble_retriever.get_relevant_documents(\"대학생 국가장학금\"))\n",
    "\n",
    "###prompt 수정 필요\n",
    "result = chain.run(\"당신은 한국의 복지 전문가입니다. 주어진 정보만을 가지고, 다음의 질문에 대답하면 됩니다. 질문 : 농촌 풍수해 피해의 경우 보상받을 수 있는 방법은? 답변 : ... \")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | StrOutputParser()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seah",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
